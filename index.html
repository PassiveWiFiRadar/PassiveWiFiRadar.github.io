<!DOCTYPE html>
<html lang="en">
<head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project | ECE, Virginia Tech | Fall 2025: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">
  
<style>
body {
  padding: 60px 20px 20px 20px;
  font-family: Arial, sans-serif;
  line-height: 1.6;
}
.container {
  max-width: 1000px;
  margin: 0 auto;
}
.page-header {
  border-bottom: 2px solid #eee;
  padding-bottom: 20px;
  margin-bottom: 30px;
}
h1 {
  color: #333;
  margin-bottom: 10px;
}
h2 {
  color: #555;
  margin-top: 30px;
  margin-bottom: 15px;
}
h3 {
  color: #666;
  margin-top: 25px;
  margin-bottom: 10px;
}
.vis {
  color: #3366CC;
}
.data {
  color: #FF9900;
}
footer {
  margin-top: 50px;
  padding-top: 20px;
  border-top: 1px solid #eee;
  color: #777;
}
table {
  border-collapse: collapse;
  margin: 20px 0;
  width: 100%;
}
th, td {
  border: 1px solid #ddd;
  padding: 8px;
  text-align: left;
}
th {
  background-color: #f2f2f2;
}
.figure-container {
  text-align: center;
  margin: 30px 0;
}
.figure-container img {
  max-width: 100%;
  height: auto;
  border: 1px solid #ddd;
  padding: 10px;
  background: #f9f9f9;
}
.caption {
  font-style: italic;
  color: #666;
  margin-top: 10px;
  font-size: 14px;
}
</style>
</head>

<body>
<div class="container">
<div class="page-header">
  <h1>Computer Vision with Passive WiFi Radar</h1> 
  <span style="font-size: 20px; line-height: 1.5em;"><strong>Sean Stafford, Austin Roberts, Jackson Moody</strong></span><br>
  <span style="font-size: 18px; line-height: 1.5em;">Fall 2025 ECE 4554/5554 Computer Vision: Course Project</span><br>
  <span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
</div>

<!-- Abstract -->
<h3>Abstract</h3>
Wireless sensing offers a privacy-preserving alternative to camera-based computer vision, yet datasets from passive WiFi radar (PWR) systems remain underexplored in vision contexts. In this project, Doppler spectrograms derived from WiFi reflections are treated as three-channel images and analyzed using convolutional neural networks for human activity recognition. We develop a training pipeline for OperaNET Doppler data, run baseline and deeper CNN models, and explore generalization to unseen subjects. Preliminary results indicate that standard computer vision architectures can meaningfully classify activities from non-visual RF data with accuracy significantly above random chance.

<br><br>

<!-- Teaser Figure -->
<h3>Teaser Figure</h3>
<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="mainfig.png">
  </div>
  <div class="caption">
    Treating passive WiFi radar Doppler spectrograms like images enables classic computer-vision models to classify human activities without cameras.
  </div>
</div>

<br><br>

<!-- Introduction -->
<h3>Introduction</h3>
This project explores the idea that WiFi can act as a "new camera" by converting Doppler frequency shifts captured by passive WiFi radar into images suitable for deep convolutional neural networks.

<br><br>

Camera-based human activity recognition (HAR) has matured, but applying similar techniques to RF sensing offers advantages in environments where cameras are undesirable or infeasible, such as private homes, hospitals, or obstructed spaces. Recent work in multimodal RF datasets, including the OperaNET dataset, suggests that Doppler spectrograms contain rich signatures of human motion. We build on this insight using standard computer-vision workflows, examining whether CNNs trained on Doppler spectrograms can classify activities like walking, running, standing, and sitting.

<br><br>


  
<br><br>

<h3>Background</h3>
  <br><br>
Passive radar is used to estimate the distance, direction, and velocity of a target without the active transmission of any RF signal. This is accomplished by taking advantage of other transmitting devices in the area, referred to as illuminators of opportunity, whose RF energy reflects off ("illuminates") the target. Passive radar is always bistatic or multistatic by definition because transmission and reception necessarily occur at different locations, with different devices. Commonly, one receiver is dedicated to receiving the direct signal from the illuminator as a timing reference, while one or more separate receivers are used to detect targets. We refer to these receivers as the reference and surveillance channels, respectively. Suppression algorithms are used to remove as much of the direct signal from the surveillance channel(s) as possible.

  <br><br>
  To estimate target range, we measure the time delay observed between receiving the direct signal via the reference channel and the reflected paths received by the surveillance channel(s). The reflected path delays are proportional to the total path distance from transmitter to target to receiver, referred to as the bistatic range [2]. However, the achievable range resolution is proportional to the bandwidth of the illuminator. In monostatic (active) pulsed radar, the range resolution is given by c/2B, where c is the speed of light in meters per second and B is the signal bandwidth in Hertz [2]. In bistatic radar, range resolution is generally limited further by certain angles in the bistatic radar geometry [2], but this is beyond the scope of our interest.
  <br><br>
  Target direction can be estimated if directional antennas are employed to measure the angle of arrival, although this is often limited to a very coarse resolution in practice. Any frequency offset between the direct and reflected signals can also be measured as the rate of change in phase shift. This rate is proportional to the velocity at which the target is moving toward or away from the baseline. Integration time determines the Doppler resolution of the system--longer is better. It is often practical to have very fine Doppler resolution on the order of 1 Hz [2]. 
<br><br>
The Doppler shift detected by passive radar may be represented as a spectrogram image, which can then be processed with computer vision techniques. Typically, the horizontal axis of the spectrogram represents time and the vertical axis represents the discrete Doppler frequency bins. Pixel intensity is proportional to the power of the received signal at the given Doppler shift. Each surveillance channel produces its own spectrogram. The reference channel only exists as a reference and is not directly used for detecting targets.

<h2>The Dataset</h2>
The dataset used in this experiment is OPERAnet [1], a multimodal dataset for HAR. Although the dataset includes WiFi CSI, Ultra-Wideband, and Microsoft X-box Kinect sensor data, our interest lies in the passive WiFi radar data subset. The hardware setup employed in [1] uses a USRP-2945 as a four-channel passive receiver with directional antennas. The first channel is used as the reference channel, recording the signal of opportunity; the remaining three are surveillance channels. The study used an Intel NUC device to create the signal of opportunity, although any stationary WiFi transmitter could be used. Refer to [1] for details regarding room layout and antenna placement.

<br><br>
  Significant preprocessing was performed on the raw PWR data prior to the dataset publication. The reference and surveillance signals were correlated to obtain Doppler frequency-shift information with 200 frequency bins of resolution, formatted as a 3 x 200 set of values at each sampling time instant [1]. The recording sample rate was 10 Hz.
  <br><br>
We chose to exclude relative range data from the dataset due to the limited 40-MHz WiFi signal bandwidth of the 5 GHz band, which provides only 3.75 meters of range resolution [1]. The 5 GHz band was chosen over the 2.4 GHz band, whose 20 MHz bandwidth would have halved the already-limited resolution. It is worth noting that the latest WiFi (802.11ax) standard is capable of up to 160 MHz channel bandwidth, providing a theoretical 0.938 meters of range resolution for potential future investigation.

  <br><br>
  A total of 38 separate experiment files containing the PWR data were provided in .mat format, including the activity labels, experiment number, person ID and room number. The activities recorded include "background", "bodyrotate", "sit", "stand", "standfromlie", "walk", "liedown", and "noactivity". These labels are stored as string values in the dataset. The label with the shortest total recording duration was "background", at 18.3465 minutes, while the longest was "noactivity" (steady state), at 124.86 minutes [1]. As we will see later, this results in a considerable imbalance that will be corrected in our preprocessing.


<!-- Approach -->
<h3>Preprocessing</h3>
Our data processing and classification pipeline was implemented in Python on Google Colab. We modified a provided Python-based visualization script provided with the dataset to convert the Doppler frequency shift data to spectrograms. Since most received power appears in the middle 100 bins, only Doppler bins 50 to 150 were kept. The linear power values are converted to decibel values. With 38 PWR experiments available to us, the output of this stage provided 38 sets of three spectrograms, each 100 pixels in height and 10 * N_s pixels in length, where N_s is the length in seconds of the experiment. TODO: add pictures of example spectrograms.
<br><br>
  We segment each spectrogram into four-second chunks by scanning over the columns of each spectrogram in steps of 10 pixels and identifying four-second-long windows whose labels are at least 60% the same (at least 24 of the 40 image columns). We do not require 100% of the labels to match one activity because some of them, such as "sit" and "stand", rarely take four seconds to occur. Activities are guarded by brief periods of the "noactivity" label before and after they occur, meaning that the subject is briefly motionless between activities. The output at this stage is thousands of spectrograms, each 100 pixels in height and 40 pixels in width. TODO: Table of label distribution before class balancing (the existing code already produces this info automatically, just need to put it into a nice tabular format.)
  <br><br>
With the spectrograms segmented into short, manageable windows for the model, we perform class balancing to uniformly remove samples of overrepresented classes and eliminate certain unwanted classes altogether. The maximum number of spectrograms per class per channel was empirically capped at 500. As shown in the histogram below (TODO: add histogram, which is also auto-generated by the code), we tolerate a moderate class inbalance but avoid having three to four times as many instances of some classes over others. We removed the "background" class and, unlike the original study [1], decided to keep the "noactivity" class in the analysis. This is because, in a real deployment, long stretches of "noactivity" are expected when no one is present, and should be classified accordingly. Thus, we have seven classes.
  <br><br>
The final preprocessing step was to convert the string activity labels to integers: '0' for "bodyrotate", '1' for "liedown", and so on, up to 6. The stratified split as follows: 20% for the test set, 20% for the validation set, and 60% for the training set. 

<h2>Baseline Model</h2>
Our baseline is a standard CNN implemented using PyTorch with four convolutional layers, ReLU activations, maxpooling, batch normalization and a fully connected classifier head. The number of feature map outputs is 16, 32, 64, and 128 for the four layers, respectively. The linear classifier neural network has two fully connected layers with 128 and 7 outputs, respectively.  Our optimizer is Adam, using the cross-entropy loss function. We validated the selection of several hyperparameters, including learning rate, weight decay, dropout, and batch size with a simple, small grid search. This model provides a reference point for later architectural complexity and allows us to replicate the core findings from the OperaNET PWR paper.

  
  <br><br>

<h3>Approach: MODIFIED model 1</h3>
  <h3>Approach: MODIFIED model 2</h3>
  
A practical challenge emerged immediately: the room number and personID metadata are stored in MATLAB-specific formats that we were unable to parse in our Google CoLab environment. We used MATLAB to export the data in a non-proprietary format before proceeding with our CoLab data processing.
After resolving metadata loading, spectrograms are normalized, resized to a consistent tensor shape, optionally augmented (noise, small shifts), and split into train/validation/test sets.

<br><br>

We also incorporate an additional experimental design: <strong>leave-one-subject-out testing</strong>. By withholding certain personIDs from training, we evaluate the model's ability to generalize to humans it has never "seen" during training—a realistic and psychologically interesting challenge for WiFi-based sensing.

<br><br>



<br><br>

<h2>Extended Architectures</h2>
After training the baseline model, deeper or pretrained networks will be tested, including deeper custom CNNs, ResNet-like architectures, and MobileNet-style small vision backbones. These models are adapted to accept 3-channel spectrogram tensors.

<br><br>

<h2>Code and Tools</h2>
We write custom preprocessing, dataset loaders, visualization, and training loops in PyTorch. Additional tools include OpenCV for preprocessing, TensorFlow (optional comparison), scipy.io for .mat extraction, and Google Colab GPUs for training.

<br><br>

<!-- Experiments and Results -->
<h3>Experiments and Results</h3>

<h2>Dataset Composition</h2>
<strong>OperaNET PWR Doppler dataset</strong>
<div class="section">
        <h2>Dataset Composition and Preprocessing</h2>
        <div class="intro-text">
            The OperaNET PWR dataset required substantial preprocessing to address severe class imbalance and remove irrelevant activity categories. This section details the transformation from raw windowed spectrograms to a balanced, training-ready dataset.
        </div>
    </div>

    <div class="section">
        <h3>Initial Distribution (Before Balancing)</h3>
        <ul>
            <li><strong>Total samples:</strong> <span class="stat-box">23,916</span> windowed spectrograms
                <div class="explanation">Each sample represents a 4-second window (40 time steps at 10 Hz) of three-channel Doppler spectrogram data</div>
            </li>
            
            <li><span class="warning">Highly imbalanced</span> with "noactivity" dominating at <span class="highlight">13,867 samples (58%)</span>
                <div class="explanation">The "noactivity" class represents steady-state periods when subjects are motionless between activities. This extreme imbalance occurred because subjects spent significant time stationary while the experimenters prepared for the next activity</div>
            </li>
            
            <li>Activity sample counts ranged dramatically from <span class="highlight">162 samples</span> (6_persons_tag_leftarm) to <span class="highlight">3,537 samples</span> (walk)
                <div class="explanation">This 22× difference between minimum and maximum class sizes would cause the model to be biased toward predicting overrepresented classes, particularly "noactivity" and "walk"</div>
            </li>
            
            <li>Dataset contained <span class="warning">unwanted calibration and tagging activities</span>:
                <ul class="nested-list">
                    <li>1_persons_tag_leftarm through 6_persons_tag_leftarm (tagging sequences where experimenters identified subjects)</li>
                    <li>Background_start (calibration measurements with no subjects present)</li>
                </ul>
                <div class="explanation">These classes represented experimental procedures rather than actual human activities of interest, and were removed entirely from the final dataset</div>
            </li>
        </ul>
    </div>

    <div class="section">
        <h3>Final Balanced Distribution (After Preprocessing)</h3>
        <ul>
            <li><strong>Total samples:</strong> <span class="stat-box">3,064</span> windowed spectrograms
                <div class="explanation">Reduced from 23,916 to 3,064 (87% reduction) by removing unwanted classes and applying class balancing. This ensures the model learns from class patterns rather than class frequencies</div>
            </li>
            
            <li><span class="success">7 activity classes retained</span> representing meaningful human activities
                <div class="explanation">These seven classes cover the core activities of interest for human activity recognition in indoor environments</div>
            </li>
            
            <li><strong>Class distribution after balancing:</strong>
                <ul class="nested-list">
                    <li><strong>bodyrotate:</strong> <span class="stat-box">500 samples</span>
                        <div class="explanation">Subject rotating their body in place (yaw rotation)</div>
                    </li>
                    <li><strong>liedown:</strong> <span class="stat-box">500 samples</span>
                        <div class="explanation">Transition from standing or sitting to lying horizontal</div>
                    </li>
                    <li><strong>noactivity:</strong> <span class="stat-box">500 samples</span>
                        <div class="explanation">Subject present but motionless—critical for real-world deployment to distinguish between activity and stillness</div>
                    </li>
                    <li><strong>sit:</strong> <span class="stat-box">299 samples</span>
                        <div class="explanation">Transition from standing to seated position (not capped at 500 because fewer than 500 valid windows existed)</div>
                    </li>
                    <li><strong>stand:</strong> <span class="stat-box">265 samples</span>
                        <div class="explanation">Transition from seated to standing position (limited samples due to brief activity duration)</div>
                    </li>
                    <li><strong>standfromlie:</strong> <span class="stat-box">500 samples</span>
                        <div class="explanation">Transition from lying down to standing upright</div>
                    </li>
                    <li><strong>walk:</strong> <span class="stat-box">500 samples</span>
                        <div class="explanation">Continuous walking motion with characteristic periodic Doppler signature</div>
                    </li>
                </ul>
            </li>
            
            <li><strong>Balancing strategy:</strong> Capped maximum samples per class at <span class="highlight">500</span>
                <div class="explanation">Classes with more than 500 samples were randomly downsampled. Classes with fewer samples (sit: 299, stand: 265) retained all available samples. This moderate imbalance (1.9:1 ratio) is acceptable and reflects the natural difficulty of capturing brief transitional activities</div>
            </li>
        </ul>
    </div>

    <div class="section">
        <h3>Train/Validation/Test Split</h3>
        <ul>
            <li><strong>Training set:</strong> <span class="stat-box">1,838 samples (60%)</span>
                <div class="explanation">Used for model weight optimization through backpropagation. Largest portion ensures sufficient examples for the model to learn discriminative Doppler patterns for each activity</div>
            </li>
            
            <li><strong>Validation set:</strong> <span class="stat-box">613 samples (20%)</span>
                <div class="explanation">Used for hyperparameter tuning and model selection during the grid search.</div>
            </li>
            
            <li><strong>Test set:</strong> <span class="stat-box">613 samples (20%)</span>
                <div class="explanation">Held out completely until final evaluation. Provides unbiased estimate of model generalization to unseen data. Only examined after all model development decisions were finalized</div>
            </li>
            
            
            <li><strong>Important limitation:</strong> Split performed at the <span class="warning">window level</span>, not experiment level
                <div class="explanation">The same experiment (and subject) may have windows appearing in both training and test sets. This means the model has seen similar Doppler characteristics during training, which may inflate performance estimates. True generalization to unseen subjects requires leave-one-subject-out validation (planned future work)</div>
            </li>
        </ul>
    </div>


<br>

<h2>Evaluation Metrics</h2>
Model performance will be assessed using:
<ul>
  <li>Accuracy</li>
  <li>F1 score</li>
  <li>Confusion matrix</li>
  <li>Random baseline for comparison (≈25% if 4 classes)</li>
  <li>Success criterion: ≥70% accuracy and F1</li>
</ul>

<br>

<h2>Preliminary Baseline Results</h2>
<div style="background: #f9f9f9; padding: 20px; border: 1px solid #ddd; margin: 20px 0;">
  <p style="color: #666;">[Insert table once you have results]</p>
  <p><strong>Expected trend:</strong> The baseline CNN should exceed random chance substantially, revealing that spectrogram "textures" are class-informative.</p>
</div>

<br>

<h2>Architecture Comparison</h2>
<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <p style="color: #999;">[Insert figure showing accuracy vs. model depth]</p>
  </div>
  <div class="caption">

      </div>
</div>

<br>

<h2>Subject-Generalization Test</h2>
<strong>caption:</strong> caption

<br><br>

<!-- Qualitative Results -->
<h3>Qualitative Results</h3>
Visual examples will include:
<ul>
  <li>Correctly classified spectrogram examples</li>
  <li>Misclassified examples</li>
  <li>Activity-specific Doppler patterns (e.g., walking ≠ running energy distribution)</li>
</ul>

<br>

<strong>Figures to include:</strong>
<ul>
  <li>Sample Doppler spectrograms for each activity</li>
  <li>Success case predictions</li>
  <li>Failure case predictions</li>
  <li>Confusion matrix heatmap</li>
</ul>

<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <p style="color: #999;">[Qualitative results will be inserted here]</p>
  </div>
</div>

<br><br>

<!-- Conclusion -->
<h3>Conclusion</h3>

  
<br><br>

Future improvements include:
<ul>
  <li>Running full subject-level generalization tests</li>
  <li>Experimenting with Vision Transformers (ViTs)</li>
  <li>Performing cross-modal comparisons with CSI-based models</li>
</ul>

<br><br>

<!-- References -->
<h3>References</h3>
[1] M. J. Bocus, "A Comprehensive Multimodal Activity Recognition Dataset Acquired from Radio Frequency and Vision-Based Sensors". figshare, 01-Aug-2022. doi: 10.6084/m9.figshare.c.5551209.v1.

<br><br>

<hr>
<footer> 
  <p>© Sean Stafford, Austin Roberts, Jackson Moody</p>
</footer>
</div>

</body>
</html>
