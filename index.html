<!DOCTYPE html>
<html lang="en">
<head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project | ECE, Virginia Tech | Fall 2025: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">
  
<style>
body {
  padding: 60px 20px 20px 20px;
  font-family: Arial, sans-serif;
  line-height: 1.6;
}
.container {
  max-width: 1000px;
  margin: 0 auto;
}
.page-header {
  border-bottom: 2px solid #eee;
  padding-bottom: 20px;
  margin-bottom: 30px;
}
h1 {
  color: #333;
  margin-bottom: 10px;
}
h2 {
  color: #555;
  margin-top: 30px;
  margin-bottom: 15px;
}
h3 {
  color: #666;
  margin-top: 25px;
  margin-bottom: 10px;
}
.vis {
  color: #3366CC;
}
.data {
  color: #FF9900;
}
footer {
  margin-top: 50px;
  padding-top: 20px;
  border-top: 1px solid #eee;
  color: #777;
}
table {
  border-collapse: collapse;
  margin: 20px 0;
  width: 100%;
}
th, td {
  border: 1px solid #ddd;
  padding: 8px;
  text-align: left;
}
th {
  background-color: #f2f2f2;
}
.figure-container {
  text-align: center;
  margin: 30px 0;
}
.figure-container img {
  max-width: 100%;
  height: auto;
  border: 1px solid #ddd;
  padding: 10px;
  background: #f9f9f9;
}
.caption {
  font-style: italic;
  color: #666;
  margin-top: 10px;
  font-size: 14px;
}
</style>
</head>

<body>
<div class="container">
<div class="page-header">
  <h1>Computer Vision with Passive WiFi Radar</h1> 
  <span style="font-size: 20px; line-height: 1.5em;"><strong>Sean Stafford, Austin Roberts, Jackson Moody</strong></span><br>
  <span style="font-size: 18px; line-height: 1.5em;">Fall 2025 ECE 4554/5554 Computer Vision: Course Project</span><br>
  <span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
</div>

<!-- Abstract -->
<h3>Abstract</h3>
Wireless sensing offers a privacy-preserving alternative to camera-based computer vision, yet datasets from passive WiFi radar (PWR) systems remain underexplored in vision contexts. In this project, Doppler spectrograms derived from WiFi reflections are treated as three-channel images and analyzed using convolutional neural networks for human activity recognition. We develop a training pipeline for OperaNET Doppler data, run baseline and deeper CNN models, and explore generalization to unseen subjects. Preliminary results indicate that standard computer vision architectures can meaningfully classify activities from non-visual RF data with accuracy significantly above random chance.

<br><br>

<!-- Teaser Figure -->
<h3>Teaser Figure</h3>
<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="mainfig.png">
  </div>
  <div class="caption">
    Treating passive WiFi radar Doppler spectrograms like images enables classic computer-vision models to classify human activities without cameras.
  </div>
</div>

<br><br>

<!-- Introduction -->
<h3>Introduction</h3>
This project explores the idea that WiFi can act as a "new camera" by converting Doppler frequency shifts captured by passive WiFi radar into images suitable for deep convolutional neural networks.

<br><br>

Camera-based human activity recognition (HAR) has matured, but applying similar techniques to RF sensing offers advantages in environments where cameras are undesirable or infeasible, such as private homes, hospitals, or obstructed spaces. Recent work in multimodal RF datasets, including the OperaNET dataset, suggests that Doppler spectrograms contain rich signatures of human motion. We build on this insight using standard computer-vision workflows, examining whether CNNs trained on Doppler spectrograms can classify activities like walking, running, standing, and sitting.

<br><br>


  
<br><br>

<!-- Approach -->
<h3>Approach</h3>

<h2>Data Preparation</h2>
The dataset used in this experiment is OPERAnet [1], a multimodal dataset for HAR. Although the dataset includes WiFi CSI, Ultra-Wideband, and Microsoft X-box Kinect sensor data, our interest lies in the passive WiFi radar data subset. The hardware setup employed in [1] uses a USRP-2945 as a four-channel passive receiver with directional antennas. The first channel is used as the reference channel, recording the signal of opportunity; the remaining three are surveillance channels. The study used an Intel NUC device to create the signal of opportunity, although any stationary WiFi transmitter could be used. Refer to [1] for details regarding room layout and antenna placement.

<br><br>
  Significant preprocessing was performed on the raw PWR data prior to the dataset publication. The reference and surveillance signals were correlated to obtain Doppler frequency-shift information with 200 frequency bins of resolution, formatted as a 3 x 200 set of values at each sampling time instant [1]. The recording sample rate was 10 Hz. They report that a CLEAN suppression algorithm was used to reduce direct signal interference.
  <br><br>
The authors chose to exclude relative range data from the dataset due to the limited 40-MHz WiFi signal bandwidth of the 5 GHz band, which provides only 3.75 meters of range resolution [1]. The 5 GHz band was chosen over the 2.4 GHz band, whose 20 MHz bandwidth would have halved the already-limited resolution. It is worth noting that the latest WiFi (802.11ax) standard is capable of up to 160 MHz channel bandwidth, providing a theoretical 0.938 meters of range resolution for potential future investigation.

  <br><br>
  A total of 38 separate experiment files containing the PWR data were provided in .mat format, including the activity labels, experiment number, person ID and room number. 

A practical challenge emerged immediately: the room number and personID metadata are stored in MATLAB-specific formats unparseable by Python. This requires loading the dataset in MATLAB once and exporting the metadata in a non-proprietary format (e.g., .mat v7, .csv, or .json). This preprocessing step is pending in our pipeline but critical for enabling subject-based generalization tests.

<br><br>

After resolving metadata loading, spectrograms are normalized, resized to a consistent tensor shape, optionally augmented (noise, small shifts), and split into train/validation/test sets.

<br><br>

We also incorporate an additional experimental design: <strong>leave-one-subject-out testing</strong>. By withholding certain personIDs from training, we evaluate the model's ability to generalize to humans it has never "seen" during training—a realistic and psychologically interesting challenge for WiFi-based sensing.

<br><br>

<h2>Baseline Model</h2>
Our baseline is a simple CNN implemented in PyTorch with three convolutional layers, ReLU activations, maxpooling, and a fully connected classifier head. This provides a reference point for later architectural complexity and allows us to replicate the core findings from the OperaNET PWR paper.

<br><br>

<h2>Extended Architectures</h2>
After training the baseline model, deeper or pretrained networks will be tested, including deeper custom CNNs, ResNet-like architectures, and MobileNet-style small vision backbones. These models are adapted to accept 3-channel spectrogram tensors.

<br><br>

<h2>Code and Tools</h2>
We write custom preprocessing, dataset loaders, visualization, and training loops in PyTorch. Additional tools include OpenCV for preprocessing, TensorFlow (optional comparison), scipy.io for .mat extraction, and Google Colab GPUs for training.

<br><br>

<!-- Experiments and Results -->
<h3>Experiments and Results</h3>

<h2>Dataset Composition</h2>
<strong>OperaNET PWR Doppler dataset</strong>
<!-- <ul>
  <li>Number of subjects: [insert once extracted]</li>
  <li>Activities: walking, running, sitting, standing, etc.</li>
  <li>Samples per activity: [insert]</li>
  <li>Number of spectrograms: [insert]</li>
  <li>Input shape: 3 × H × W (treated as RGB-like)</li>
  <li>Train/test splits: Standard 70/15/15 split</li>
  <li>Additional: leave-one-subject-out for generalization</li>
</ul> -->

<br>

<h2>Evaluation Metrics</h2>
Model performance will be assessed using:
<ul>
  <li>Accuracy</li>
  <li>F1 score</li>
  <li>Confusion matrix</li>
  <li>Random baseline for comparison (≈25% if 4 classes)</li>
  <li>Success criterion: ≥70% accuracy and F1</li>
</ul>

<br>

<h2>Preliminary Baseline Results</h2>
<div style="background: #f9f9f9; padding: 20px; border: 1px solid #ddd; margin: 20px 0;">
  <p style="color: #666;">[Insert table once you have results]</p>
  <p><strong>Expected trend:</strong> The baseline CNN should exceed random chance substantially, revealing that spectrogram "textures" are class-informative.</p>
</div>

<br>

<h2>Architecture Comparison</h2>
<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <p style="color: #999;">[Insert figure showing accuracy vs. model depth]</p>
  </div>
  <div class="caption">

      </div>
</div>

<br>

<h2>Subject-Generalization Test</h2>
<strong>caption:</strong> caption

<br><br>

<!-- Qualitative Results -->
<h3>Qualitative Results</h3>
Visual examples will include:
<ul>
  <li>Correctly classified spectrogram examples</li>
  <li>Misclassified examples</li>
  <li>Activity-specific Doppler patterns (e.g., walking ≠ running energy distribution)</li>
</ul>

<br>

<strong>Figures to include:</strong>
<ul>
  <li>Sample Doppler spectrograms for each activity</li>
  <li>Success case predictions</li>
  <li>Failure case predictions</li>
  <li>Confusion matrix heatmap</li>
</ul>

<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <p style="color: #999;">[Qualitative results will be inserted here]</p>
  </div>
</div>

<br><br>

<!-- Conclusion -->
<h3>Conclusion</h3>

  
<br><br>

Future improvements include:
<ul>
  <li>Resolving MATLAB metadata into Python-readable format</li>
  <li>Running full subject-level generalization tests</li>
  <li>Experimenting with Vision Transformers (ViTs)</li>
  <li>Performing cross-modal comparisons with CSI-based models</li>
</ul>

<br><br>

<!-- References -->
<h3>References</h3>
[1] M. J. Bocus, "A Comprehensive Multimodal Activity Recognition Dataset Acquired from Radio Frequency and Vision-Based Sensors". figshare, 01-Aug-2022. doi: 10.6084/m9.figshare.c.5551209.v1.

<br><br>

<hr>
<footer> 
  <p>© Sean Stafford, Austin Roberts, Jackson Moody</p>
</footer>
</div>

</body>
</html>
