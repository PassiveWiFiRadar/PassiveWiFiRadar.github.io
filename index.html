<!DOCTYPE html>
<html lang="en">
<head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project | ECE, Virginia Tech | Fall 2025: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">
  
<style>
body {
  padding: 60px 20px 20px 20px;
  font-family: Arial, sans-serif;
  line-height: 1.6;
}
.container {
  max-width: 1000px;
  margin: 0 auto;
}
.page-header {
  border-bottom: 2px solid #eee;
  padding-bottom: 20px;
  margin-bottom: 30px;
}
h1 {
  color: #333;
  margin-bottom: 10px;
}
h2 {
  color: #555;
  margin-top: 30px;
  margin-bottom: 15px;
}
h3 {
  color: #666;
  margin-top: 25px;
  margin-bottom: 10px;
}
.vis {
  color: #3366CC;
}
.data {
  color: #FF9900;
}
footer {
  margin-top: 50px;
  padding-top: 20px;
  border-top: 1px solid #eee;
  color: #777;
}
table {
  border-collapse: collapse;
  margin: 20px 0;
  width: 100%;
}
th, td {
  border: 1px solid #ddd;
  padding: 8px;
  text-align: left;
}
th {
  background-color: #f2f2f2;
}
.figure-container {
  text-align: center;
  margin: 30px 0;
}
.figure-container img {
  max-width: 100%;
  height: auto;
  border: 1px solid #ddd;
  padding: 10px;
  background: #f9f9f9;
}
.caption {
  font-style: italic;
  color: #666;
  margin-top: 10px;
  font-size: 14px;
}
</style>
</head>

<body>
<div class="container">
<div class="page-header">
  <h1>Computer Vision with Passive WiFi Radar</h1> 
  <span style="font-size: 20px; line-height: 1.5em;"><strong>Sean Stafford, Austin Roberts, Jackson Moody</strong></span><br>
  <span style="font-size: 18px; line-height: 1.5em;">Fall 2025 ECE 4554/5554 Computer Vision: Course Project</span><br>
  <span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
</div>

<!-- Abstract -->
<h3>Abstract</h3>
Wireless sensing offers a privacy-preserving alternative to camera-based computer vision, yet datasets from passive WiFi radar (PWR) systems remain underexplored in vision contexts. In this project, Doppler spectrograms derived from WiFi reflections are treated as three-channel images and analyzed using convolutional neural networks for human activity recognition. We develop a training pipeline for OperaNET Doppler data, run baseline and deeper CNN models, and explore generalization to unseen subjects. Preliminary results indicate that standard computer vision architectures can meaningfully classify activities from non-visual RF data with accuracy significantly above random chance.

<br><br>

<!-- Teaser Figure -->
<h3>Teaser Figure</h3>
<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="mainfig.png">
  </div>
  <div class="caption">
    Treating passive WiFi radar Doppler spectrograms like images enables classic computer-vision models to classify human activities without cameras.
  </div>
</div>

<br><br>

<!-- Introduction -->
<h3>Introduction</h3>
This project explores the idea that WiFi can act as a "new camera" by converting Doppler frequency shifts captured by passive WiFi radar into images suitable for deep convolutional neural networks.

<br><br>

Camera-based human activity recognition (HAR) has matured, but applying similar techniques to RF sensing offers advantages in environments where cameras are undesirable or infeasible, such as private homes, hospitals, or obstructed spaces. Recent work in multimodal RF datasets, including the OperaNET dataset, suggests that Doppler spectrograms contain rich signatures of human motion. We build on this insight using standard computer-vision workflows, examining whether CNNs trained on Doppler spectrograms can classify activities like walking, running, standing, and sitting.

<br><br>


  
<br><br>

<h3>Background</h3>
  <br><br>
Passive radar is used to estimate the distance, direction, and velocity of a target without the active transmission of any RF signal. This is accomplished by taking advantage of other transmitting devices in the area, referred to as illuminators of opportunity, whose RF energy reflects off ("illuminates") the target. Passive radar is always bistatic or multistatic by definition because transmission and reception necessarily occur at different locations, with different devices. Commonly, one receiver is dedicated to receiving the direct signal from the illuminator as a timing reference, while one or more separate receivers are used to detect targets. We refer to these receivers as the reference and surveillance channels, respectively. Suppression algorithms are used to remove as much of the direct signal from the surveillance channel(s) as possible.

  <br><br>
  To estimate target range, we measure the time delay observed between receiving the direct signal via the reference channel and the reflected paths received by the surveillance channel(s). The reflected path delays are proportional to the total path distance from transmitter to target to receiver, referred to as the bistatic range [2]. However, the achievable range resolution is proportional to the bandwidth of the illuminator. In monostatic (active) pulsed radar, the range resolution is given by c/2B, where c is the speed of light in meters per second and B is the signal bandwidth in Hertz [2]. In bistatic radar, range resolution is generally limited further by certain angles in the bistatic radar geometry [2], but this is beyond the scope of our interest.
  <br><br>
  Target direction can be estimated if directional antennas are employed to measure the angle of arrival, although this is often limited to a very coarse resolution in practice. Any frequency offset between the direct and reflected signals can also be measured as the rate of change in phase shift. This rate is proportional to the velocity at which the target is moving toward or away from the baseline. Integration time determines the Doppler resolution of the system--longer is better. It is often practical to have very fine Doppler resolution on the order of 1 Hz [2]. 
<br><br>
The Doppler shift detected by passive radar may be represented as a spectrogram image, which can then be processed with computer vision techniques. Typically, the horizontal axis of the spectrogram represents time and the vertical axis represents the discrete Doppler frequency bins. Pixel intensity is proportional to the power of the received signal at the given Doppler shift. Each surveillance channel produces its own spectrogram. The reference channel only exists as a reference and is not directly used for detecting targets.

<h2>The Dataset</h2>
The dataset used in this experiment is OPERAnet [1], a multimodal dataset for HAR. Although the dataset includes WiFi CSI, Ultra-Wideband, and Microsoft X-box Kinect sensor data, our interest lies in the passive WiFi radar data subset. The hardware setup employed in [1] uses a USRP-2945 as a four-channel passive receiver with directional antennas. The first channel is used as the reference channel, recording the signal of opportunity; the remaining three are surveillance channels. The study used an Intel NUC device to create the signal of opportunity, although any stationary WiFi transmitter could be used. Refer to [1] for details regarding room layout and antenna placement.

<br><br>
  Significant preprocessing was performed on the raw PWR data prior to the dataset publication. The reference and surveillance signals were correlated to obtain Doppler frequency-shift information with 200 frequency bins of resolution, formatted as a 3 x 200 set of values at each sampling time instant [1]. The recording sample rate was 10 Hz.
  <br><br>
We chose to exclude relative range data from the dataset due to the limited 40-MHz WiFi signal bandwidth of the 5 GHz band, which provides only 3.75 meters of range resolution [1]. The 5 GHz band was chosen over the 2.4 GHz band, whose 20 MHz bandwidth would have halved the already-limited resolution. It is worth noting that the latest WiFi (802.11ax) standard is capable of up to 160 MHz channel bandwidth, providing a theoretical 0.938 meters of range resolution for potential future investigation.

  <br><br>
  A total of 38 separate experiment files containing the PWR data were provided in .mat format, including the activity labels, experiment number, person ID and room number. The activities recorded include "background", "bodyrotate", "sit", "stand", "standfromlie", "walk", "liedown", and "noactivity". These labels are stored as string values in the dataset. The label with the shortest total recording duration was "background", at 18.3465 minutes, while the longest was "noactivity" (steady state), at 124.86 minutes [1]. As we will see later, this results in a considerable imbalance that will be corrected in our preprocessing.


<!-- Approach -->
<h3>Preprocessing</h3>
Our data processing and classification pipeline was implemented in Python on Google Colab. We modified a provided Python-based visualization script provided with the dataset to convert the Doppler frequency shift data to spectrograms. Since most received power appears in the middle 100 bins, only Doppler bins 50 to 150 were kept. The linear power values are converted to decibel values. With 38 PWR experiments available to us, the output of this stage provided 38 sets of three spectrograms, each 100 pixels in height and 10 * N_s pixels in length, where N_s is the length in seconds of the experiment. TODO: add pictures of example spectrograms.
<br><br>
  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="cvImage.png">
  </div>
  <div class="caption">

      </div>
</div>
  We segment each spectrogram into four-second chunks by scanning over the columns of each spectrogram in steps of 10 pixels and identifying four-second-long windows whose labels are at least 60% the same (at least 24 of the 40 image columns). We do not require 100% of the labels to match one activity because some of them, such as "sit" and "stand", rarely take four seconds to occur. Activities are guarded by brief periods of the "noactivity" label before and after they occur, meaning that the subject is briefly motionless between activities. The output at this stage is thousands of spectrograms, each 100 pixels in height and 40 pixels in width. TODO: Table of label distribution before class balancing (the existing code already produces this info automatically, just need to put it into a nice tabular format.)
  <br><br>
With the spectrograms segmented into short, manageable windows for the model, we perform class balancing to uniformly remove samples of overrepresented classes and eliminate certain unwanted classes altogether. The maximum number of spectrograms per class per channel was empirically capped at 500. As shown in the histogram below (TODO: add histogram, which is also auto-generated by the code), we tolerate a moderate class inbalance but avoid having three to four times as many instances of some classes over others. We removed the "background" class and, unlike the original study [1], decided to keep the "noactivity" class in the analysis. This is because, in a real deployment, long stretches of "noactivity" are expected when no one is present, and should be classified accordingly. Thus, we have seven classes.
  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="samplesGraphCV.png">
  </div>
  <div class="caption">

      </div>
</div>
  <br><br>
The final preprocessing step was to convert the string activity labels to integers: '0' for "bodyrotate", '1' for "liedown", and so on, up to 6. The stratified split as follows: 20% for the test set, 20% for the validation set, and 60% for the training set. 

<h2>Baseline Model</h2>
Our baseline is a standard CNN implemented using PyTorch with four convolutional layers, ReLU activations, maxpooling, batch normalization and a fully connected classifier head. The number of feature map outputs is 16, 32, 64, and 128 for the four layers, respectively. The linear classifier neural network has two fully connected layers with 128 and 7 outputs, respectively.  Our optimizer is Adam, using the cross-entropy loss function. We validated the selection of several hyperparameters, including learning rate, weight decay, dropout, and batch size with a simple, small grid search. This model provides a reference point for later architectural complexity and allows us to replicate the core findings from the OperaNET PWR paper.

  
  <br><br>

<h3>Approach: MODIFIED model 1 - Test On Never Before Seen Room</h3>
 The network must learn spatial–temporal patterns from Doppler spectrogram windows regardless of which room they were recorded in. The generalization question is handled strictly in the data split—training on windows from some rooms and testing on an unseen room—not in the structure of the model itself. In other words, the architecture remains a 4-layer convolutional front-end followed by a dense classifier head with seven outputs, but the training distribution shifts to evaluate environmental robustness rather than requiring any redesign of the network.
  A practical challenge emerged immediately: the room number and personID metadata are stored in MATLAB-specific formats that we were unable to parse in our Google CoLab environment. We used MATLAB to export the data in a non-proprietary format before proceeding with our CoLab data processing.
After resolving metadata loading, spectrograms are normalized, resized to a consistent tensor shape, optionally augmented (noise, small shifts), and split into train/validation/test sets.
<p>
While the baseline experiment demonstrated that CNNs can classify activities from WiFi Doppler spectrograms, it used a random train/test split where the same physical environments (and potentially the same subjects) appeared in both training and testing. This approach, while standard for evaluating general classification performance, does not address a critical real-world deployment question: <strong>Can the model generalize to new physical environments it has never encountered during training?</strong>
</p>

<p>
WiFi signal propagation is highly sensitive to environmental factors including room geometry, furniture placement, wall materials, and the presence of other objects that cause multipath reflections. Each room creates a unique "signature" in how WiFi signals scatter and reflect. A model that performs well on data from rooms it has seen during training may struggle when deployed in a new environment where these propagation characteristics differ significantly.
</p>

<p>
To test true environmental generalization, we designed an experiment where <strong>all data from Room 2 is held out as a test set</strong>, while the model is trained exclusively on data from all other rooms (Rooms 1, 3, and 4). This "leave-one-room-out" approach provides a realistic assessment of how the model would perform when deployed in a new location, which is essential for practical applications like elderly care monitoring, smart home systems, or security applications where the system must work in diverse environments.
</p>

<h3>Experimental Design</h3>

<h4>Data Splitting Strategy</h4>
<p>
Unlike the baseline experiment which performed a random 60/20/20 split across all data, the room generalization experiment enforces a strict spatial separation:
</p>

<table class="comparison-table">
  <thead>
    <tr>
      <th>Dataset</th>
      <th>Source Rooms</th>
      <th>Number of Experiments</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Training Set</strong></td>
      <td>Rooms 1, 3, 4</td>
      <td>31 experiments → 6,340 windows</td>
      <td>Model learns activity patterns</td>
    </tr>
    <tr>
      <td><strong>Validation Set</strong></td>
      <td>Rooms 1, 3, 4</td>
      <td>Same 31 experiments → 1,586 windows (20% split)</td>
      <td>Hyperparameter tuning, model selection</td>
    </tr>
    <tr style="background-color: #fffacd;">
      <td><strong>Test Set (Room 2)</strong></td>
      <td><strong>Room 2 ONLY</strong></td>
      <td>6 experiments → 1,770 windows</td>
      <td><strong>Evaluate generalization to unseen environment</strong></td>
    </tr>
  </tbody>
</table>

<div class="interpretation">
<strong>Key Design Decision:</strong> The 31 experiments from non-Room-2 environments were first separated, then randomly split 80/20 into training and validation sets. This ensures both training and validation come from the same distribution (rooms 1, 3, 4), while Room 2 represents a completely independent test distribution. This mimics real-world deployment where validation data helps tune the model, but ultimate performance is measured on genuinely new environments.
</div>

<h4>Preprocessing Differences</h4>
<p>
The preprocessing pipeline remained largely identical to the baseline experiment, with one important modification:
</p>

<ul>
  <li><strong>Windowing:</strong> Same 4-second windows (40 frames at 10 Hz) with 1-second stride (10 frames)</li>
  <li><strong>Frequency bins:</strong> Same selection of bins 50-150 (middle 100 bins with highest power)</li>
  <li><strong>Class balancing:</strong> Applied to training/validation data, capping most classes at 2,000 samples per class</li>
  <li><strong>Test set handling:</strong> Room 2 test data was NOT balanced—it retains the natural class distribution to reflect realistic deployment conditions</li>
</ul>

<div class="warning-box">
<strong>Important Note:</strong> We excluded "noactivity" from this experiment by filtering windows with >70% noactivity labels. This decision was made because:
<ol style="margin-top: 10px;">
  <li>The extreme imbalance in noactivity samples (dominating the dataset) made training unstable</li>
  <li>Our focus was on distinguishing between different <em>active</em> human motions rather than motion vs. stillness</li>
  <li>This reduced the classification problem to 6 classes: bodyrotate, liedown, sit, stand, standfromlie, and walk</li>
</ol>
</div>

<h4>Class Distribution</h4>
<p>
After preprocessing and class balancing, the final dataset composition was:
</p>

<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>Training/Validation Count</th>
      <th>Room 2 Test Count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>bodyrotate</td>
      <td>2,000 (balanced)</td>
      <td>340</td>
    </tr>
    <tr>
      <td>liedown</td>
      <td>905</td>
      <td>260</td>
    </tr>
    <tr>
      <td>sit</td>
      <td>1,070</td>
      <td>264</td>
    </tr>
    <tr>
      <td>stand</td>
      <td>1,061</td>
      <td>283</td>
    </tr>
    <tr>
      <td>standfromlie</td>
      <td>890</td>
      <td>265</td>
    </tr>
    <tr>
      <td>walk</td>
      <td>2,000 (balanced)</td>
      <td>358</td>
    </tr>
    <tr style="font-weight: bold; background-color: #e8f4f8;">
      <td><strong>Total</strong></td>
      <td><strong>7,926</strong></td>
      <td><strong>1,770</strong></td>
    </tr>
  </tbody>
</table>

  
  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="sa.png">
  </div>
  <div class="caption">
    
      </div>
  <h3>Approach: MODIFIED model 2</h3>
  
<h3>Approach: MODIFIED model 2 - Distinguish between different people</h3>

One interesting question we had was this: if a PWR can be used to accurately distinguish between different types of activity, could it also be used to distinguish between different people? This model was almost identical to the baseline in terms of methodology. The main difference was that instead of using activity labels, we trained the model with the personID labels. The people in the experiments were simply labeled with strings from “One” to “Seven.” In the preprocessing, we excluded experiment_028, since that one has a slightly different methodology and doesn’t include accurate personID. Unlike the baseline model, we also increased the maximum samples per class to 1400, which can be seen in the image below. 

  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="class_dist_room_experiment.png">
  </div>
  <div class="caption">
    
      </div>
</div>

For the model itself, we used the exact same CNN as the baseline. After training, we evaluated the accuracy on the test loader to be 74.18%, which is quite impressive considering the difficulty of the problem. The confusion matrix below also shows which people the model found easier to identify and which ones were more difficult.

  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="personIDconfusion.png">
  </div>
  <div class="caption">

      </div>
</div>

  
<br><br>


<!-- Experiments and Results -->
<h3>Experiments and Results</h3>

<h2>Dataset Composition</h2>
<strong>OperaNET PWR Doppler dataset</strong>
<div class="section">
        <h2>Dataset Composition and Preprocessing</h2>
        <div class="intro-text">
            The OperaNET PWR dataset required substantial preprocessing to address severe class imbalance and remove irrelevant activity categories. This section details the transformation from raw windowed spectrograms to a balanced, training-ready dataset.
        </div>
    </div>

    <div class="section">
        <h3>Initial Distribution (Before Balancing)</h3>
        <ul>
            <li><strong>Total samples:</strong> <span class="stat-box">23,916</span> windowed spectrograms
                <div class="explanation">Each sample represents a 4-second window (40 time steps at 10 Hz) of three-channel Doppler spectrogram data</div>
            </li>
            
            <li><span class="warning">Highly imbalanced</span> with "noactivity" dominating at <span class="highlight">13,867 samples (58%)</span>
                <div class="explanation">The "noactivity" class represents steady-state periods when subjects are motionless between activities. This extreme imbalance occurred because subjects spent significant time stationary while the experimenters prepared for the next activity</div>
            </li>
            
            <li>Activity sample counts ranged dramatically from <span class="highlight">162 samples</span> (6_persons_tag_leftarm) to <span class="highlight">3,537 samples</span> (walk)
                <div class="explanation">This 22× difference between minimum and maximum class sizes would cause the model to be biased toward predicting overrepresented classes, particularly "noactivity" and "walk"</div>
            </li>
            
            <li>Dataset contained <span class="warning">unwanted calibration and tagging activities</span>:
                <ul class="nested-list">
                    <li>1_persons_tag_leftarm through 6_persons_tag_leftarm (tagging sequences where experimenters identified subjects)</li>
                    <li>Background_start (calibration measurements with no subjects present)</li>
                </ul>
                <div class="explanation">These classes represented experimental procedures rather than actual human activities of interest, and were removed entirely from the final dataset</div>
            </li>
        </ul>
    </div>

    <div class="section">
        <h3>Final Balanced Distribution (After Preprocessing)</h3>
        <ul>
            <li><strong>Total samples:</strong> <span class="stat-box">3,064</span> windowed spectrograms
                <div class="explanation">Reduced from 23,916 to 3,064 (87% reduction) by removing unwanted classes and applying class balancing. This ensures the model learns from class patterns rather than class frequencies</div>
            </li>
            
            <li><span class="success">7 activity classes retained</span> representing meaningful human activities
                <div class="explanation">These seven classes cover the core activities of interest for human activity recognition in indoor environments</div>
            </li>
            
            <li><strong>Class distribution after balancing:</strong>
                <ul class="nested-list">
                    <li><strong>bodyrotate:</strong> <span class="stat-box">500 samples</span>
                        <div class="explanation">Subject rotating their body in place (yaw rotation)</div>
                    </li>
                    <li><strong>liedown:</strong> <span class="stat-box">500 samples</span>
                        <div class="explanation">Transition from standing or sitting to lying horizontal</div>
                    </li>
                    <li><strong>noactivity:</strong> <span class="stat-box">500 samples</span>
                        <div class="explanation">Subject present but motionless—critical for real-world deployment to distinguish between activity and stillness</div>
                    </li>
                    <li><strong>sit:</strong> <span class="stat-box">299 samples</span>
                        <div class="explanation">Transition from standing to seated position (not capped at 500 because fewer than 500 valid windows existed)</div>
                    </li>
                    <li><strong>stand:</strong> <span class="stat-box">265 samples</span>
                        <div class="explanation">Transition from seated to standing position (limited samples due to brief activity duration)</div>
                    </li>
                    <li><strong>standfromlie:</strong> <span class="stat-box">500 samples</span>
                        <div class="explanation">Transition from lying down to standing upright</div>
                    </li>
                    <li><strong>walk:</strong> <span class="stat-box">500 samples</span>
                        <div class="explanation">Continuous walking motion with characteristic periodic Doppler signature</div>
                    </li>
                </ul>
            </li>
            
            <li><strong>Balancing strategy:</strong> Capped maximum samples per class at <span class="highlight">500</span>
                <div class="explanation">Classes with more than 500 samples were randomly downsampled. Classes with fewer samples (sit: 299, stand: 265) retained all available samples. This moderate imbalance (1.9:1 ratio) is acceptable and reflects the natural difficulty of capturing brief transitional activities</div>
            </li>
        </ul>
    </div>

    <div class="section">
        <h3>Train/Validation/Test Split</h3>
        <ul>
            <li><strong>Training set:</strong> <span class="stat-box">1,838 samples (60%)</span>
                <div class="explanation">Used for model weight optimization through backpropagation. Largest portion ensures sufficient examples for the model to learn discriminative Doppler patterns for each activity</div>
            </li>
            
            <li><strong>Validation set:</strong> <span class="stat-box">613 samples (20%)</span>
                <div class="explanation">Used for hyperparameter tuning and model selection during the grid search.</div>
            </li>
            
            <li><strong>Test set:</strong> <span class="stat-box">613 samples (20%)</span>
                <div class="explanation">Held out completely until final evaluation. Provides unbiased estimate of model generalization to unseen data. Only examined after all model development decisions were finalized</div>
            </li>
            
            
            <li><strong>Important limitation:</strong> Split performed at the <span class="warning">window level</span>, not experiment level
                <div class="explanation">The same experiment (and subject) may have windows appearing in both training and test sets. This means the model has seen similar Doppler characteristics during training, which may inflate performance estimates. True generalization to unseen subjects requires leave-one-subject-out validation (planned future work)</div>
            </li>
        </ul>
    </div>


<br>

<h2>Evaluation Metrics</h2>
<div class="section">
        <h2>Evaluation Metrics</h2>
        <div class="intro-text">
            Model performance is assessed using multiple complementary metrics that provide different perspectives on classification quality. Our evaluation strategy includes standard classification metrics, confusion matrix analysis, and a novel confidence-based evaluation approach tailored to the characteristics of WiFi-based activity recognition.
        </div>
    </div>

    <div class="section">
        <h3>Primary Metrics</h3>

        <div class="metric-card">
            <h4>1. Classification Accuracy</h4>
            <div class="formula">
                Accuracy = (Number of Correct Predictions) / (Total Number of Predictions)
            </div>
            <div class="explanation">
                <strong>Purpose:</strong> Overall measure of how often the model correctly identifies activities across all seven classes.
            </div>
            <ul>
                <li><strong>Computed separately</strong> for training, validation, and test sets
                    <div class="explanation">Training accuracy shows how well the model fits the training data. Validation accuracy guides hyperparameter selection. Test accuracy provides the final unbiased performance estimate.</div>
                </li>
                <li><strong>Baseline comparison:</strong> Random guessing baseline = <span class="highlight">14.3%</span> (1/7 classes)
                    <div class="explanation">Any meaningful model must substantially exceed this threshold. We target ≥70% accuracy to demonstrate practical utility.</div>
                </li>
                
            </ul>
        </div>

        <div class="metric-card">
            <h4>2. Confusion Matrix</h4>
            <div class="explanation">
                <strong>Purpose:</strong> Provides detailed per-class performance breakdown, revealing which activities are easily distinguished and which are commonly confused.
            </div>
            <ul>
                <li><strong>Structure:</strong> 7×7 matrix where rows represent true labels and columns represent predicted labels
                    <div class="explanation">Diagonal elements show correct classifications. Off-diagonal elements reveal misclassification patterns.</div>
                </li>
                <li><strong>Reveals critical patterns:</strong>
                    <ul class="nested-list">
                        <li><strong>Transitional activity confusion:</strong> "sit" vs "stand" vs "standfromlie" may show cross-confusion due to similar Doppler signatures during brief movements</li>
                        <li><strong>Motion vs stillness:</strong> "walk" and "bodyrotate" should be clearly separated from "noactivity"</li>
                        <li><strong>Class-specific performance:</strong> Some activities may have consistently high recall while others struggle</li>
                    </ul>
                </li>

            </ul>

            <div class="warning-box">
                <strong>Expected Confusion Patterns:</strong> Activities with similar motion profiles (e.g., "sit" and "stand" both involve vertical torso movement) may show higher confusion rates. This is physically meaningful rather than a model failure.
            </div>
        </div>

        <div class="metric-card">
            <h4>3. Per-Class Metrics</h4>
            
            <div class="formula">
                Precision = True Positives / (True Positives + False Positives)
            </div>


            <div class="formula" style="margin-top: 15px;">
                Recall = True Positives / (True Positives + False Negatives)
            </div>


            <div class="formula" style="margin-top: 15px;">
                F1-Score = 2 × (Precision × Recall) / (Precision + Recall)
            </div>

            <ul>
                <li><strong>Application to our dataset:</strong>
                    <ul class="nested-list">
                        <li>Classes with fewer samples (sit, stand) may show lower recall</li>
                        <li>Common activities (walk, bodyrotate) should show balanced precision/recall</li>
                        <li>F1-scores reveal which classes need more training data or architectural improvements</li>
                    </ul>
                </li>
            </ul>
        </div>
    </div>

    <div class="section">
        <h3>Hyperparameter Optimization Metrics</h3>

        <div class="metric-card">
            <h4>6. Validation Accuracy During Grid Search</h4>
            <ul>
                <li><strong>Grid search parameters:</strong>
                    <ul class="nested-list">
                        <li>Learning rate: [1e-4, 3e-4, 1e-3]</li>
                        <li>Weight decay: [0, 1e-4, 1e-3]</li>
                        <li>Dropout: [0, 0.1, 0.3]</li>
                        <li>Batch size: [32, 64, 128]</li>
                        <li><strong>Total combinations:</strong> 81</li>
                    </ul>
                </li>
                <li><strong>Selection criterion:</strong> Configuration achieving highest validation accuracy after 12 epochs
                    <div class="explanation">Validation set is used exclusively for this selection to prevent test set leakage. Each configuration is trained from scratch to ensure fair comparison.</div>
                </li>
                <li><strong>Reported results:</strong> Best hyperparameters and corresponding validation/test performance
                    <div class="explanation">This demonstrates the improvement gained through systematic hyperparameter optimization vs using default values.</div>
                </li>
            </ul>
        </div>
    </div>

    <div class="section">
        <h3>Baseline Comparisons</h3>

        <table class="comparison-table">
            <thead>
                <tr>
                    <th>Baseline</th>
                    <th>Expected Performance</th>
                    <th>Purpose</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Random Guessing</strong></td>
                    <td>14.3% (1/7 classes)</td>
                    <td>Absolute minimum performance threshold</td>
                </tr>
                <tr>
                    <td><strong>Majority Class</strong></td>
                    <td>~16.3% (always predict "bodyrotate", "liedown", "noactivity", "standfromlie", or "walk")</td>
                    <td>Shows benefit of learning vs exploiting dataset bias (prevented by class balancing)</td>
                </tr>
                <tr>
                    <td><strong>Shallow CNN (Baseline Model)</strong></td>
                    <td>Target: ≥70%</td>
                    <td>Demonstrates feasibility of CNN-based approach for RF spectrograms</td>
                </tr>
                <tr>
                    <td><strong>Deeper Architectures (Future)</strong></td>
                    <td>Expected: ≥75%</td>
                    <td>Quantify benefit of architectural complexity</td>
                </tr>
            </tbody>
        </table>
    </div>

    <div class="section">
        <h3>Success Criteria</h3>
        <div class="success-box">
            <strong>Minimum Viable Performance:</strong>
            <ul style="margin-top: 10px;">
                <li>Test accuracy ≥ 70%</li>
                <li>No single class with F1-score < 0.5</li>
                <li>Training-test accuracy gap < 15% (acceptable generalization)</li>
                <li>Validation accuracy guides hyperparameter selection effectively</li>
            </ul>
        </div>

    </div>

<br>

<h2>Preliminary Baseline Results</h2>
<div style="background: #f9f9f9; padding: 20px; border: 1px solid #ddd; margin: 20px 0;">
  <p style="color: #666;">[Insert table once you have results]</p>
  <p><strong>Expected trend:</strong> The baseline CNN should exceed random chance substantially, revealing that spectrogram "textures" are class-informative.</p>
</div>

<br>

<h2>Architecture Comparison</h2>


<br>

<h2>Subject-Generalization Test</h2>
<strong>caption:</strong> caption

<br><br>

<!-- Qualitative Results -->
<h3>Qualitative Results</h3>
Visual examples will include:
<ul>
  <li>Correctly classified spectrogram examples</li>
  <li>Misclassified examples</li>
  <li>Activity-specific Doppler patterns (e.g., walking ≠ running energy distribution)</li>
</ul>

<br>

<strong>Figures to include:</strong>
<ul>
  <li>Sample Doppler spectrograms for each activity</li>
  <li>Success case predictions</li>
  <li>Failure case predictions</li>
  <li>Confusion matrix heatmap</li>
</ul>

<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <p style="color: #999;">[Qualitative results will be inserted here]</p>
  </div>
</div>

<br><br>

<!-- Conclusion -->
<h3>Conclusion</h3>

  
<br><br>

Future improvements include:
<ul>
  <li>Running full subject-level generalization tests</li>
  <li>Experimenting with Vision Transformers (ViTs)</li>
  <li>Performing cross-modal comparisons with CSI-based models</li>
</ul>

<br><br>

<!-- References -->
<h3>References</h3>
[1] M. J. Bocus, "A Comprehensive Multimodal Activity Recognition Dataset Acquired from Radio Frequency and Vision-Based Sensors". figshare, 01-Aug-2022. doi: 10.6084/m9.figshare.c.5551209.v1.

<br><br>

<hr>
<footer> 
  <p>© Sean Stafford, Austin Roberts, Jackson Moody</p>
</footer>
</div>

</body>
</html>
