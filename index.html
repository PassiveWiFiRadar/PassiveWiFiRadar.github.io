<!DOCTYPE html>
<html lang="en">
<head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project | ECE, Virginia Tech | Fall 2025: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">
  
<style>
body {
  padding: 60px 20px 20px 20px;
  font-family: Arial, sans-serif;
  line-height: 1.6;
}
.container {
  max-width: 1000px;
  margin: 0 auto;
}
.page-header {
  border-bottom: 2px solid #eee;
  padding-bottom: 20px;
  margin-bottom: 30px;
}
h1 {
  color: #333;
  margin-bottom: 10px;
}
h2 {
  color: #555;
  margin-top: 30px;
  margin-bottom: 15px;
}
h3 {
  color: #666;
  margin-top: 25px;
  margin-bottom: 10px;
}
.vis {
  color: #3366CC;
}
.data {
  color: #FF9900;
}
footer {
  margin-top: 50px;
  padding-top: 20px;
  border-top: 1px solid #eee;
  color: #777;
}
table {
  border-collapse: collapse;
  margin: 20px 0;
  width: 100%;
}
th, td {
  border: 1px solid #ddd;
  padding: 8px;
  text-align: left;
}
th {
  background-color: #f2f2f2;
}
.figure-container {
  text-align: center;
  margin: 30px 0;
}
.figure-container img {
  max-width: 100%;
  height: auto;
  border: 1px solid #ddd;
  padding: 10px;
  background: #f9f9f9;
}
.caption {
  font-style: italic;
  color: #666;
  margin-top: 10px;
  font-size: 14px;
}
</style>
</head>

<body>
<div class="container">
<div class="page-header">
  <h1>Computer Vision with Passive WiFi Radar</h1> 
  <span style="font-size: 20px; line-height: 1.5em;"><strong>Sean Stafford, Austin Roberts, Jackson Moody</strong></span><br>
  <span style="font-size: 18px; line-height: 1.5em;">Fall 2025 ECE 4554/5554 Computer Vision: Course Project</span><br>
  <span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
</div>

<!-- Abstract -->
<h1>Abstract</h1>
Wireless sensing offers a privacy-preserving alternative to camera-based computer vision, yet datasets from passive WiFi radar (PWR) systems remain underexplored in vision contexts. In this project, Doppler spectrograms derived from WiFi reflections are treated as three-channel images and analyzed using convolutional neural networks (CNNs) for human activity recognition. We develop a training pipeline for OperaNET Doppler data, run baseline human activity classification, and explore some variations of this problem. Results indicate that a simple CNN can meaningfully classify activities from non-visual RF data with accuracy generally exceeding 70% for five of six classes of human activity.

<br><br>

<!-- Teaser Figure -->
<h3>Teaser Figure</h3>
<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="mainfig.png">
  </div>
  <div class="caption">
    Treating passive WiFi radar Doppler spectrograms like images enables classic computer-vision models to classify human activities without cameras. This figure was obtained from [1].
  </div>
</div>

<br><br>

<!-- Introduction -->
<h1>Introduction</h1>
Computer vision techniques are not limited to processing camera data, nor is camera deployment always feasible in areas where privacy is of concern, or when opaque objects may block views. It is implied but not often discussed in computer vision courses that image processing techniques may be applied to any 2D signal representation, including spectrograms from wireless signals. By representing signals as images, we can process them with convolutional neural networks (CNNs) and similar computer vision techniques similarly to images from a camera. Wireless sensing offers a promising approach to detecting various features about the environment in situations where cameras may not be appropriate, such as in private homes.
  <br><br>
  Wireless sensing is a very broad field, but our interests are constrained to everyday wireless signals--specifically WiFi. Although WiFi is not designed for integrated sensing and communication, it remains a convenient choice for certain sensing applications because of its ubiquity when compared to expensive, specialized radio hardware.
  <br><br>
  There are two dominant methods for exploiting WiFi signals for wireless sensing: via channel state information (CSI) and via passive wifi radar (PWR) [1]. The most practical difference between the two is that the CSI-based approach requires digital access to the WiFi modem system or firmware, while the PWR approach merely requires physical access to the room, as well as to more specialized hardware.
  <br><br>
  The CSI approach exploits the channel estimation process that is fundamental to modern wireless communications under a simple assumption: that humans in the environment disturb the wireless channel in ways that modern wireless receivers must estimate and correct for, to successfully detect and demodulate a message. When a message is transmitted over the wireless channel, the radio frequency energy reflects off objects in the environment, taking paths with varying distances before arriving at the receiver. This phenomenon is called multipath. Each path arrives at the receiver with a random phase shift due to the differences in path length, causing constructive and destructive interference. The receiver generally observes amplitude and phase distortion of the signal that varies across frequency, known as frequency-selective fading, which corrupts the message. To combat this, the transmitter sends known pilot signals across time or frequency to the receiver. By comparing the received pilot to the expected pilot, the receiver can determine the amplitude and phase distortion effects of the channel--the CSI--and apply an inverse filtering operation on subsequent messages. Thus, CSI contains implicit information about the environment that can be exploited for sensing human activities [1]. However, obtaining this data from a standard WiFi modem requires access to low-level firmware.
  <br><br>
  The PWR approach is perhaps simpler in principle, utilizing existing WiFi signals in the environment as an illuminator of opportunity and monitoring their reflections in the environment with separate, dedicated radio hardware that passively listens. Thus, no digital access to any WiFi device is required, but physical access to the room is required for placement of the passive receiver antenna(s). The subsequent Background section provides a more thorough explanation of this approach.
  
<br><br>
The goal of this project is to explore the use of WiFi as a unique form of computer vision by exploiting the way the physical environment interacts with wireless signals. Specifically, we intend to use Doppler spectrograms from a PWR dataset for human activity recognition (HAR). We chose the PWR approach because it tends to be less popular than the CSI approach.

<br><br>

The remainder of this report is structured as follows: a brief background on passive radar fundamentals is provided, followed by a detailed description of the dataset used in this project. We then present our baseline CNN and data processing pipeline. Three experiments are explored: basic activity classification, cross-room generalization, and person identification.
<br><br>

<h1>Background</h1>
Passive radar is used to estimate the distance, direction, and velocity of a target without the active transmission of any RF signal. This is accomplished by taking advantage of other transmitting devices in the area--illuminators of opportunity--whose RF energy reflects off ("illuminates") the target. Passive radar is always bistatic or multistatic by definition because transmission and reception necessarily occur at different locations, with different devices. Commonly, one receiver channel is dedicated to receiving the direct signal from the illuminator as a timing reference, while one or more additional receiver channels are used to detect targets. We refer to these receivers as the reference and surveillance channels, respectively. Suppression algorithms are used to remove as much of the direct signal from the surveillance channel(s) as possible.

  <br><br>
  To estimate target range, we measure the time delay observed between receiving the direct signal via the reference channel and the reflected paths received by the surveillance channel(s). The reflected path delays are proportional to the total path distance from transmitter to target to receiver, referred to as the bistatic range [2]. However, the achievable range resolution is proportional to the bandwidth of the illuminator. In monostatic (active) pulsed radar, the range resolution is given by c/2B, where c is the speed of light in meters per second and B is the signal bandwidth in Hertz [2]. In bistatic radar, range resolution is generally limited further by certain angles in the bistatic radar geometry [2], but this is beyond the scope of our interest.
  <br><br>
  Target direction can be estimated if directional antennas are employed to measure the angle of arrival, although this is often limited to a very coarse resolution in practice [2]. Any frequency offset between the direct and reflected signals can also be measured as the rate of change in phase shift. This rate is proportional to the velocity at which the target is moving toward or away from the baseline. Integration time determines the Doppler resolution of the system--longer is better. It is often practical to have very fine Doppler resolution on the order of 1 Hz [2]. 
<br><br>
The Doppler shift detected by passive radar may be represented as a spectrogram image, which can then be processed with computer vision techniques. Typically, the horizontal axis of the spectrogram represents time and the vertical axis represents the discrete Doppler frequency bins. Pixel intensity is proportional to the power of the received signal at the given Doppler shift. Each surveillance channel produces its own spectrogram. The reference channel only exists as a reference and is not directly used for detecting targets.

<h1>The Dataset</h1>
The dataset used in this experiment is OPERAnet [1], a multimodal dataset for HAR. Although the dataset includes WiFi CSI, Ultra-Wideband, and Microsoft X-box Kinect sensor data, our interest lies in the passive WiFi radar data subset. The hardware setup employed in [1] to capture the PWR data involves a USRP-2945 as a four-channel passive receiver with directional antennas. The first channel is used as the reference channel, recording the signal of opportunity; the remaining three are surveillance channels. The study used an Intel NUC device to create the signal of opportunity, although any stationary WiFi transmitter could be used. Refer to [1] for details regarding room layout and antenna placement.

<br><br>
  Significant preprocessing was performed on the raw PWR data prior to the dataset publication. The reference and surveillance signals were correlated to obtain Doppler frequency-shift information with 200 frequency bins of resolution, formatted as a 3 x 200 set of values at each sampling time instant [1]. The recording sample rate was 10 Hz.
  <br><br>
The creators of OPERAnet chose to exclude relative range data from the dataset due to the limited 40-MHz WiFi signal bandwidth of the 5 GHz band, which provides only 3.75 meters of range resolution [1]. The 5 GHz band was chosen over the 2.4 GHz band, whose 20 MHz bandwidth would have halved the already-limited resolution. 

  <br><br>
  A total of 38 separate experiment files containing the PWR data were provided in .mat format, including the activity labels, experiment number, person ID and room number. The activities recorded include "background", "bodyrotate", "sit", "stand", "standfromlie", "walk", "liedown", and "noactivity". These labels are stored as string values in the dataset. The label with the shortest total recording duration was "background", at 18.3465 minutes, while the longest was "noactivity" (steady state), at 124.86 minutes [1]. The dataset includes a crowd-counting experiment in which the labels are not human activities, but rather the number of people present in the room. We excluded this experiment file from our pipeline.


<h1>Experiment 1: Human Activity Classification Baseline </h1>
<h2>Preprocessing</h2>
Our data processing and classification pipeline was implemented in Python on Google Colab. We modified a Python-based visualization script provided with the dataset [1] to convert the Doppler frequency shift data to spectrograms. Since most received power appears in the middle 100 bins, only Doppler bins 50 to 150 were kept. The linear power values are converted to decibel values. With 38 PWR experiments available to us, the output of this stage provided 37 sets of three spectrograms (excluding the crowd-counting experiment), each 100 pixels in height and 10 * N_s pixels in length, where N_s is the length in seconds of the experiment. 
<br><br>
  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="doppler_spectrogram_exp10.png">
  </div>
  <div class="caption">
Example Doppler spectrogram of Experiment 10: intermittent walking.
      </div>
</div>
  The first step of our preprocessing pipeline was to split the dataset into training, testing, and validation sets. Rather than achieve an exact percentage split, we split the dataset by experiment file. This ensures that the model is tested solely on data from experiments it has never seen before. However, the experiment durations vary, and while we aimed for a 60%-20%-20% split of experiment files across the training, testing, and validation sets respectively, the true distribution of spectrogram samples is more uneven.  
  <br><br>
  We segment each spectrogram into four-second chunks by scanning over the columns of each spectrogram in steps of 10 pixels and identifying four-second-long windows for which at least 30% of the corresponding labels contain an activity other than "noactivity". We do not require 100% of the labels to match one activity because some of them, such as "sit" and "stand", rarely take four seconds to occur. Activities are guarded by brief periods of the "noactivity" label before and after they occur, meaning that the subject is briefly motionless between activities. The output at this stage is thousands of spectrograms, each 100 pixels in height and 40 pixels in width. 
  <br><br>
With the spectrograms segmented into short, manageable windows for the model, we perform class balancing to uniformly remove samples of overrepresented classes and eliminate certain unwanted classes altogether. The maximum number of spectrograms per class per channel was empirically capped at 2000. As shown in the histogram below, we tolerate a moderate class inbalance but avoid having three to four times as many instances of some classes over others. We removed the "background" class, as data with this label was only available from a single experiment. Thus, we have six classes. The total number of spectrograms in the training set was 6988; for the test set, it was 2444. The test set distribution was left as-is.
  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="baseline_sample_dist.png">
  </div>
  <div class="caption">
Training dataset label distribution for the baseline HAR model, with slight adjustments to keep class imbalances manageable.
      </div>
</div>
  <br><br>

    <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="baseline_TEST_set_distribution.png">
  </div>
  <div class="caption">
Test dataset label distribution for the baseline HAR model.
      </div>
</div>
  <br><br>
The final preprocessing step was to convert the string activity labels to integers: '0' for "bodyrotate", '1' for "liedown", and so on, up to 5.

<h2>The Model</h2>
Our baseline is a standard CNN adapted from Homework 5, implemented using PyTorch with three convolutional layers, ReLU activations, maxpooling, batch normalization and a fully connected classifier head. The number of feature map outputs is 16, 32, and 64 for the three layers, respectively. The number of layers and feature maps was tuned manually through trial and error. The linear classifier neural network has two fully connected layers with 128 and 6 outputs, respectively.  Our optimizer is Adam, using the cross-entropy loss function. We automatically validated the selection of several hyperparameters, including learning rate, weight decay, dropout, and batch size with a simple, small grid search.

  <h2>Experiment 1 Results</h2>
  Results indicate a test accuracy of 77.4%, with a per-class classification accuracy at or exceeding 70% for all classes except liedown, with an accuracy of 50%. The model has some apparent confusion distinguishing between sitting and lying down, which is reasonable given the similarity of the motions. Walking was the most accurately predicted class of the six by a wide margin at 91%, likely because it produces particularly strong Doppler shifts. Overall, the model performance is clearly better than random guessing, which would have an accuracy around 17%. With a training accuracy of 98.8%, we suspect there is some lingering overfitting, although this is somewhat reduced by hyperparameter selection.
  <br><br>
    <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="baseline_confusion_matrix.png">
  </div>
  <div class="caption">
    Confusion matrix for baseline activity classification accuracy.
      </div>
</div>

The F1 scores--a combined measure of precision and recall performance--closely follow the confusion matrix, with "walk" having the highest score at 0.918 and liedown with the lowest at 0.534. Note that precision is a measure of how often the model is correct when it chooses a particular class. Recall is simply the number of true test instances that the model correctly classifies. 

  <br><br>
  <table border="1" cellpadding="6" cellspacing="0">
  <thead>
    <tr>
      <th>Class</th>
      <th>F1 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>bodyrotate</td>
      <td>0.804</td>
    </tr>
    <tr>
      <td>liedown</td>
      <td>0.534</td>
    </tr>
    <tr>
      <td>sit</td>
      <td>0.640</td>
    </tr>
    <tr>
      <td>stand</td>
      <td>0.706</td>
    </tr>
    <tr>
      <td>standfromlie</td>
      <td>0.645</td>
    </tr>
    <tr>
      <td>walk</td>
      <td>0.918</td>
    </tr>
  </tbody>
</table>

These scores indicate that strong, repetitive Doppler patterns like "bodyrotate" and "walk" are much better captured than the more transient activities.
  <br><br>
  Finally, the optimal hyperparameters and corresponding validation accuracy are given below. Note that learning rate refers to the step size used in gradient descent as the model updates its weights and attempts to find a good minimum close to the global one; a larger learning rate means larger steps, potentially speeding up convergence at the risk of overshooting the minimum and causing divergence. Weight decay is a form of regularization that penalizes large weights, which in a CNN translates to less aggressive filters. This can help reduce overfitting. The batch size is the number of training windows passed to the model before each weight update cycle. Finally, dropout randomly sets some proportion of the activations in the fully connected network to zero to improve overfitting.
  <!-- Accuracy Table -->
<table style="border-collapse:collapse; width:100%; font-family:Arial, sans-serif; margin-bottom:16px;">
  <thead>
    <tr>
      <th colspan="2" style="border:1px solid #ddd; padding:10px; text-align:left; background:#f0f0f0; font-size:14px;">
        Model Performance
      </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border:1px solid #ddd; padding:8px; width:60%;"><strong>Validation accuracy</strong></td>
      <td style="border:1px solid #ddd; padding:8px; width:40%;">84.68&nbsp;%</td>
    </tr>
  </tbody>
</table>

<!-- Best Hyperparameters Table -->
<table style="border-collapse:collapse; width:100%; font-family:Arial, sans-serif;">
  <thead>
    <tr>
      <th colspan="2" style="border:1px solid #ddd; padding:10px; text-align:left; background:#f0f0f0; font-size:14px;">
        Best Hyperparameters
      </th>
    </tr>
    <tr>
      <th style="border:1px solid #eee; padding:6px; text-align:left; background:#fafafa;">Param</th>
      <th style="border:1px solid #eee; padding:6px; text-align:left; background:#fafafa;">Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border:1px solid #eee; padding:6px;"><strong>learning_rate</strong></td>
      <td style="border:1px solid #eee; padding:6px;">0.001</td>
    </tr>
    <tr>
      <td style="border:1px solid #eee; padding:6px;"><strong>weight_decay</strong></td>
      <td style="border:1px solid #eee; padding:6px;">0.0001</td>
    </tr>
    <tr>
      <td style="border:1px solid #eee; padding:6px;"><strong>dropout</strong></td>
      <td style="border:1px solid #eee; padding:6px;">0.1</td>
    </tr>
    <tr>
      <td style="border:1px solid #eee; padding:6px;"><strong>batch_size</strong></td>
      <td style="border:1px solid #eee; padding:6px;">32</td>
    </tr>
  </tbody>
</table>

  <br><br>
<h1>Experiment 2: Room Generalization </h1>
<p>
While the baseline experiment demonstrated that CNNs can classify activities from WiFi Doppler spectrograms, it used a random train/test split by experiment, where the same physical rooms (and the same subjects) appeared in both training and testing. This approach, while standard for evaluating general classification performance, does not address whether the model generalizes to new physical environments it has never encountered during training.  The network must learn spatial–temporal patterns from Doppler spectrogram windows regardless of which room they were recorded in.
</p>
<h2>Approach</h2>
<p>
To test true environmental generalization, we designed an experiment where <strong>all data from Room 2 is held out as a test set</strong>, while the model is trained exclusively on data from Room 1. This "leave-one-room-out" approach provides a realistic assessment of how the model would perform when deployed in a new location, which is essential for practical applications like elderly care monitoring, smart home systems, or security applications where the system must work in diverse environments. 
Initially, the room number and personID metadata are stored in MATLAB-specific formats that we were unable to parse in our Google CoLab environment. We used MATLAB to export the data in a non-proprietary format before proceeding with our CoLab data processing. After resolving metadata loading, spectrograms are normalized, resized to a consistent tensor shape, and split into train/validation/test sets.
</p>

<div class="interpretation">
The 31 experiments from non-Room-2 environments were first separated, then randomly split 80/20 into training and validation sets. This ensures both training and validation come from the same distribution (Room 1), while Room 2 represents a completely independent test distribution. This mimics real-world deployment where validation data helps tune the model, but ultimate performance is measured on genuinely new environments. Room 2 test data was NOT balanced, so it retains the natural class distribution to reflect realistic deployment conditions.
</div>


<h3>Class Distribution</h3>
  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="class_dist_room_experiment.png">
  </div>
    
  <div class="caption">
    The final dataset composition after preprocessing and class balancing
    </div>
    </div>
    <h2>Experiment 2 Results</h2>
  Results indicate a test accuracy of 68.70%, with a per-class classification accuracy at or exceeding 68% for all classes except sit, with an accuracy of 53%. The model still has some apparent confusion distinguishing between sitting and lying down. Walking was the most accurately predicted class of the six again, by a wide margin at 89%. Overall, the model performance is clearly better than random guessing even in a new room environment, which would have an accuracy as mentioned before (around 17%). 

    <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="training_matrix.png">
  </div>
  <div class="caption">
    Confusion Matrix on the unseen Room 2
  </div>
  </div>

  <table border="1" cellpadding="6" cellspacing="0">
  <thead>
    <tr>
      <th>Class</th>
      <th>F1 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>bodyrotate</td>
      <td>0.879</td>
    </tr>
    <tr>
      <td>liedown</td>
      <td>0.700</td>
    </tr>
    <tr>
      <td>sit</td>
      <td>0.647</td>
    </tr>
    <tr>
      <td>stand</td>
      <td>0.730</td>
    </tr>
    <tr>
      <td>standfromlie</td>
      <td>0.678</td>
    </tr>
    <tr>
      <td>walk</td>
      <td>0.923</td>
    </tr>
  </tbody>
</table>

These scores are consistent with the baseline model. The F1 scores closely follow the confusion matrix, with "walk" having the highest score at 0.923 and sit with the lowest at 0.647
  <br>
  
  <!-- Accuracy Table -->
<table style="border-collapse:collapse; width:100%; font-family:Arial, sans-serif; margin-bottom:16px;">
  <thead>
    <tr>
      <th colspan="2" style="border:1px solid #ddd; padding:10px; text-align:left; background:#f0f0f0; font-size:14px;">
        Model Performance
      </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border:1px solid #ddd; padding:8px; width:60%;"><strong>Validation accuracy</strong></td>
      <td style="border:1px solid #ddd; padding:8px; width:40%;">83.10&nbsp;%</td>
    </tr>
      <td style="border:1px solid #ddd; padding:8px; width:60%;"><strong>Test accuracy</strong></td>
      <td style="border:1px solid #ddd; padding:8px; width:40%;">68.70&nbsp;%</td>
    
  </tbody>
</table>
  

<h1>Experiment 3: Person Identification</h1>

Another interesting question we had was this: if a PWR can be used to accurately distinguish between different types of activity, could it also be used to distinguish between different people? This model was almost identical to the baseline in terms of methodology. The main difference was that instead of using activity labels, we trained the model with the personID labels. The people in the experiments were simply labeled with strings from “One” to “Six.” Unlike the baseline model, we also reduced the maximum samples per class to 1400 for stricter class balancing, which can be seen in the image below.

  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="personIDclassdist.png">
  </div>
  <div class="caption">
  Class distribution for personID model, all activity types included
      </div>
</div>
<h2>Experiment 3 Results</h2>
For the model itself, we used the exact same CNN as the baseline. After training, we evaluated the accuracy on the test loader to be 74.18%. However, this result comes with an enormous qualification: to achieve this, we perform a stratified train-test-validation split of 60%/20%/20% across all experiments. This is quite different from the baseline, where experiments from the test set were excluded from the training set. 

  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="personIDconfusion.png">
  </div>
  <div class="caption">
  Confusion matrix for personID model, with data leak
      </div>
</div>

Upon reflection, it was quite clear that the data leakage was artificially inflating our testing accuracy. In other words, the model wasn’t learning how to recognize people, it was just learning how to distinguish between the different experiments. We attempted to separate the sets by experiment and found that, despite attempts at regularization, training accuracy often exceeded 98% while test accuracy often was around 20% (random guessing). In an effort to improve this, we removed any spectrogram window that was greater than 30% “noactivity,” since obviously that model won’t be able to identify a person if there is no person there to identify. This didn’t offer any improvements, so we removed all spectrograms from the dataset other than "walk" samples, under the presumption that walking is more unique to each individual than any of the other classes. See the class distribution below. Clearly, there were far fewer datapoints for the “walk” activity than for the baseline. 

  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="personIDclassdist_walk.png">
  </div>
  <div class="caption">
  Class Distribution for "walk" activity only
      </div>
</div>
  
<br><br>

However, this also had a minimal impact on results. Our conclusion was that PWR data is insufficient to adequately distinguish between different people. As you may notice in the confusion matrix below, we also couldn’t include person 4 in the test set, since there was only one experiment that included them walking. We considered splitting the experiment, but then we would likely have the same issue with data leakage as before and it would just recognize person 4 based on the experiment. If we were to further expand on this experiment, it would probably be necessary to create a much more expansive dataset particularly suited for this purpose.

  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="personIDconfusion_walk.png">
  </div>
  <div class="caption">
  Confusion matrix for personID model, "walk" activity only
      </div>
</div>


  
<br>

<!-- Qualitative Results -->
<h1>Qualitative Results</h1>


<br>
Below is a snapshot of spectrograms from the test set of the baseline model (Experiment 1). They are labeled with the true class label, followed by the predicted label. We observe that some samples from different classes look visually very similar to each other, potentially explaining some of the mistaken classifications. All images come from PWR surveillance channel 1--channels 2 and 3 are not shown.

<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="qualitative_results.png">
  </div>
</div>

Below is a snapshot of spectrograms from the test set of the personID model (Experiment 3). Unlike the baseline model, the differences between the spectrograms are much less substantial, and this clarifies as to why the model was not able to perform.

  <div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="personID_freq.png">
  </div>
  <div class="caption">
  Spectrogram Snapshot from the PersonID model (Experiment 3)
      </div>
</div>
  
<br><br>

<!-- Conclusion -->
<h1>Conclusion</h1>
Overall, this project successfully demonstrates that HAR with PWR Doppler spectrograms is generally possible with accuracy well above random chance, and even generalizes well to a new room that the model has never seen before. While some classes, such as liedown, were significantly harder to identify than more obvious motions, such as walking, the confusion matrices consistently show results that are far better than random guessing. F1 scores also indicate that the network is learning meaningful patterns rather than memorizing. The qualitative results show that this is indeed a difficult classification task, with different activities appearing almost indistinguishable from each other in the spectrogram images. For future work, it may be interesting to incorporate a vision transformer to handle the temporal data, particularly on a larger dataset if one becomes available.


<br><br>

<!-- References -->
<h2>References</h2>
[1] M. J. Bocus, "A Comprehensive Multimodal Activity Recognition Dataset Acquired from Radio Frequency and Vision-Based Sensors". figshare, 01-Aug-2022. doi: 10.6084/m9.figshare.c.5551209.v1.
<br><br>
[2] H Griffiths and C. J. Baker, An introduction to passive radar. Boston: Artech House, 2017.
 <br><br>
  <b>Borrowed Code:</b> We borrowed small portions of code from class Homework 5 to build and train the CNN, as well as a small portion of code by Chong Tang provided with the OPERAnet dataset used for creating the spectrograms.
<br><br>
<b>Use of Generative AI.</b> The authors disclose that GPT-5.1 was used to assist in this project. All code and analysis was verified by the authors, who take full responsibility for the content.
<hr>
<footer> 
  <p>© Sean Stafford, Austin Roberts, Jackson Moody</p>
</footer>
</div>

</body>
</html>
