<!DOCTYPE html>
<html lang="en">
<head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project | ECE, Virginia Tech | Fall 2025: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">
  
<style>
body {
  padding: 60px 20px 20px 20px;
  font-family: Arial, sans-serif;
  line-height: 1.6;
}
.container {
  max-width: 1000px;
  margin: 0 auto;
}
.page-header {
  border-bottom: 2px solid #eee;
  padding-bottom: 20px;
  margin-bottom: 30px;
}
h1 {
  color: #333;
  margin-bottom: 10px;
}
h2 {
  color: #555;
  margin-top: 30px;
  margin-bottom: 15px;
}
h3 {
  color: #666;
  margin-top: 25px;
  margin-bottom: 10px;
}
.vis {
  color: #3366CC;
}
.data {
  color: #FF9900;
}
footer {
  margin-top: 50px;
  padding-top: 20px;
  border-top: 1px solid #eee;
  color: #777;
}
table {
  border-collapse: collapse;
  margin: 20px 0;
  width: 100%;
}
th, td {
  border: 1px solid #ddd;
  padding: 8px;
  text-align: left;
}
th {
  background-color: #f2f2f2;
}
.figure-container {
  text-align: center;
  margin: 30px 0;
}
.figure-container img {
  max-width: 100%;
  height: auto;
  border: 1px solid #ddd;
  padding: 10px;
  background: #f9f9f9;
}
.caption {
  font-style: italic;
  color: #666;
  margin-top: 10px;
  font-size: 14px;
}
</style>
</head>

<body>
<div class="container">
<div class="page-header">
  <h1>Computer Vision with Passive WiFi Radar</h1> 
  <span style="font-size: 20px; line-height: 1.5em;"><strong>Sean Stafford, Austin Roberts, Jackson Moody</strong></span><br>
  <span style="font-size: 18px; line-height: 1.5em;">Fall 2025 ECE 4554/5554 Computer Vision: Course Project</span><br>
  <span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
</div>

<!-- Abstract -->
<h3>Abstract</h3>
Wireless sensing offers a privacy-preserving alternative to camera-based computer vision, yet datasets from passive WiFi radar (PWR) systems remain underexplored in vision contexts. In this project, Doppler spectrograms derived from WiFi reflections are treated as three-channel images and analyzed using convolutional neural networks for human activity recognition. We develop a training pipeline for OperaNET Doppler data, run baseline and deeper CNN models, and explore generalization to unseen subjects. Preliminary results indicate that standard computer vision architectures can meaningfully classify activities from non-visual RF data with accuracy significantly above random chance.

<br><br>

<!-- Teaser Figure -->
<h3>Teaser Figure</h3>
<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <img style="height: 500px;" alt="" src="mainfig.png">
  </div>
  <div class="caption">
    Treating passive WiFi radar Doppler spectrograms like images enables classic computer-vision models to classify human activities without cameras.
  </div>
</div>

<br><br>

<!-- Introduction -->
<h3>Introduction</h3>
This project explores the idea that WiFi can act as a "new camera" by converting Doppler frequency shifts captured by passive WiFi radar into images suitable for deep convolutional neural networks.

<br><br>

Camera-based human activity recognition (HAR) has matured, but applying similar techniques to RF sensing offers advantages in environments where cameras are undesirable or infeasible, such as private homes, hospitals, or obstructed spaces. Recent work in multimodal RF datasets, including the OperaNET dataset, suggests that Doppler spectrograms contain rich signatures of human motion. We build on this insight using standard computer-vision workflows, examining whether CNNs trained on Doppler spectrograms can classify activities like walking, running, standing, and sitting.

<br><br>


  
<br><br>



<h2>The Dataset</h2>
The dataset used in this experiment is OPERAnet [1], a multimodal dataset for HAR. Although the dataset includes WiFi CSI, Ultra-Wideband, and Microsoft X-box Kinect sensor data, our interest lies in the passive WiFi radar data subset. The hardware setup employed in [1] uses a USRP-2945 as a four-channel passive receiver with directional antennas. The first channel is used as the reference channel, recording the signal of opportunity; the remaining three are surveillance channels. The study used an Intel NUC device to create the signal of opportunity, although any stationary WiFi transmitter could be used. Refer to [1] for details regarding room layout and antenna placement.

<br><br>
  Significant preprocessing was performed on the raw PWR data prior to the dataset publication. The reference and surveillance signals were correlated to obtain Doppler frequency-shift information with 200 frequency bins of resolution, formatted as a 3 x 200 set of values at each sampling time instant [1]. The recording sample rate was 10 Hz. They report that a CLEAN suppression algorithm was used to reduce direct signal interference.
  <br><br>
The authors chose to exclude relative range data from the dataset due to the limited 40-MHz WiFi signal bandwidth of the 5 GHz band, which provides only 3.75 meters of range resolution [1]. The 5 GHz band was chosen over the 2.4 GHz band, whose 20 MHz bandwidth would have halved the already-limited resolution. It is worth noting that the latest WiFi (802.11ax) standard is capable of up to 160 MHz channel bandwidth, providing a theoretical 0.938 meters of range resolution for potential future investigation.

  <br><br>
  A total of 38 separate experiment files containing the PWR data were provided in .mat format, including the activity labels, experiment number, person ID and room number. The activities recorded include "background", "bodyrotate", "sit", "stand", "standfromlie", "liedown", and "noactivity". These labels are stored as string values in the dataset. The label with the shortest total recording duration was "background", at 18.3465 minutes, while the longest was "noactivity" (steady state), at 124.86 minutes [1]. As we will see later, this results in a considerable imbalance that will be corrected in our preprocessing.


<!-- Approach -->
<h3>Baseline Model</h3>
Our data processing and classification pipeline was implemented in Python on Google Colab. We modified a provided Python-based visualization script provided with the dataset to convert the Doppler frequency shift data to spectrograms. Since most received power appears in the middle 100 bins, only Doppler bins 50 to 150 were kept. The linear power values are converted to decibel values. With 38 PWR experiments available to us, the output of this stage provided 38 sets of three spectrograms, each 100 pixels in height and 10 * N_s pixels in length, where N_s is the length in seconds of the experiment. 
<br><br>
  We segment each spectrogram into four-second chunks by scanning over the columns of each spectrogram in steps of 10 pixels and identifying four-second-long windows whose labels are at least 60% the same. We do not require 100% of the labels to match one activity because some of them, such as "sit", and "stand", rarely take four seconds to occur. Activities are guarded by brief periods of the "noactivity" label before and after they occur, meaning that the subject is briefly motionless between activities. The output at this stage is thousands of spectrograms, each 100 pixels in height and 40 pixels in width. TODO: Table of label distribution before class balancing (the existing code already produces this info automatically, just need to put it into a nice tabular format.)
  <br><br>
With the spectrograms segmented into short, manageable windows for the model, we perform class balancing to uniformly remove samples of overrepresented classes and eliminate certain unwanted classes altogether. The maximum number of spectrograms per class per channel was empirically capped at 500. As shown in the histogram below (TODO: add histogram, which is also auto-generated by the code), we tolerate a moderate class inbalance but avoid having three to four times as many instances of some classes over others. We removed the "background" class and, unlike the original study [1], decided to keep the "noactivity" class in the analysis. This is because, in a real deployment, long stretches of "noactivity" are expected when no one is present, and should be classified accordingly. Thus, we have seven classes.
  <br><br>
The final preprocessing step was to convert the string activity labels to integers: '0' for "bodyrotate", '1' for "liedown", and so on, up to 6. The dataset is then split as follows: 20% for the test set, 20% for the validation set, and 60% for the training set. 

<h2>Baseline Model</h2>
Our baseline is a standard CNN implemented using PyTorch with four convolutional layers, ReLU activations, maxpooling, batch normalization and a fully connected classifier head. The number of feature map outputs is 16, 32, 64, and 128 for the four layers, respectively. The linear classifier neural network has two fully connected layers with 128 and 7 outputs, respectively.  Our optimizer is Adam, using the cross-entropy loss function. We validated the selection of several hyperparameters, including learning rate, weight decay, dropout, and batch size with a simple, small grid search. This model provides a reference point for later architectural complexity and allows us to replicate the core findings from the OperaNET PWR paper.

  
  <br><br>

<h3>Approach: MODIFIED model 1</h3>
  <h3>Approach: MODIFIED model 2</h3>
  
A practical challenge emerged immediately: the room number and personID metadata are stored in MATLAB-specific formats that we were unable to parse in our Google CoLab environment. We used MATLAB to export the data in a non-proprietary format before proceeding with our CoLab data processing.
After resolving metadata loading, spectrograms are normalized, resized to a consistent tensor shape, optionally augmented (noise, small shifts), and split into train/validation/test sets.

<br><br>

We also incorporate an additional experimental design: <strong>leave-one-subject-out testing</strong>. By withholding certain personIDs from training, we evaluate the model's ability to generalize to humans it has never "seen" during training—a realistic and psychologically interesting challenge for WiFi-based sensing.

<br><br>



<br><br>

<h2>Extended Architectures</h2>
After training the baseline model, deeper or pretrained networks will be tested, including deeper custom CNNs, ResNet-like architectures, and MobileNet-style small vision backbones. These models are adapted to accept 3-channel spectrogram tensors.

<br><br>

<h2>Code and Tools</h2>
We write custom preprocessing, dataset loaders, visualization, and training loops in PyTorch. Additional tools include OpenCV for preprocessing, TensorFlow (optional comparison), scipy.io for .mat extraction, and Google Colab GPUs for training.

<br><br>

<!-- Experiments and Results -->
<h3>Experiments and Results</h3>

<h2>Dataset Composition</h2>
<strong>OperaNET PWR Doppler dataset</strong>
<!-- <ul>
  <li>Number of subjects: [insert once extracted]</li>
  <li>Activities: walking, running, sitting, standing, etc.</li>
  <li>Samples per activity: [insert]</li>
  <li>Number of spectrograms: [insert]</li>
  <li>Input shape: 3 × H × W (treated as RGB-like)</li>
  <li>Train/test splits: Standard 70/15/15 split</li>
  <li>Additional: leave-one-subject-out for generalization</li>
</ul> -->

<br>

<h2>Evaluation Metrics</h2>
Model performance will be assessed using:
<ul>
  <li>Accuracy</li>
  <li>F1 score</li>
  <li>Confusion matrix</li>
  <li>Random baseline for comparison (≈25% if 4 classes)</li>
  <li>Success criterion: ≥70% accuracy and F1</li>
</ul>

<br>

<h2>Preliminary Baseline Results</h2>
<div style="background: #f9f9f9; padding: 20px; border: 1px solid #ddd; margin: 20px 0;">
  <p style="color: #666;">[Insert table once you have results]</p>
  <p><strong>Expected trend:</strong> The baseline CNN should exceed random chance substantially, revealing that spectrogram "textures" are class-informative.</p>
</div>

<br>

<h2>Architecture Comparison</h2>
<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <p style="color: #999;">[Insert figure showing accuracy vs. model depth]</p>
  </div>
  <div class="caption">

      </div>
</div>

<br>

<h2>Subject-Generalization Test</h2>
<strong>caption:</strong> caption

<br><br>

<!-- Qualitative Results -->
<h3>Qualitative Results</h3>
Visual examples will include:
<ul>
  <li>Correctly classified spectrogram examples</li>
  <li>Misclassified examples</li>
  <li>Activity-specific Doppler patterns (e.g., walking ≠ running energy distribution)</li>
</ul>

<br>

<strong>Figures to include:</strong>
<ul>
  <li>Sample Doppler spectrograms for each activity</li>
  <li>Success case predictions</li>
  <li>Failure case predictions</li>
  <li>Confusion matrix heatmap</li>
</ul>

<div class="figure-container">
  <div style="background: #f0f0f0; padding: 40px; border: 1px solid #ddd;">
    <p style="color: #999;">[Qualitative results will be inserted here]</p>
  </div>
</div>

<br><br>

<!-- Conclusion -->
<h3>Conclusion</h3>

  
<br><br>

Future improvements include:
<ul>
  <li>Resolving MATLAB metadata into Python-readable format</li>
  <li>Running full subject-level generalization tests</li>
  <li>Experimenting with Vision Transformers (ViTs)</li>
  <li>Performing cross-modal comparisons with CSI-based models</li>
</ul>

<br><br>

<!-- References -->
<h3>References</h3>
[1] M. J. Bocus, "A Comprehensive Multimodal Activity Recognition Dataset Acquired from Radio Frequency and Vision-Based Sensors". figshare, 01-Aug-2022. doi: 10.6084/m9.figshare.c.5551209.v1.

<br><br>

<hr>
<footer> 
  <p>© Sean Stafford, Austin Roberts, Jackson Moody</p>
</footer>
</div>

</body>
</html>
